{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItXfxkxvosLH"
      },
      "source": [
        "# Introduction to Binary Text Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eg62Pmz3o83v"
      },
      "source": [
        "\n",
        "**Text classification** is the processs of training a model to recognize key features from text, and then classify or assign a label to that text. In this project, we will be using a **binary classifier** to decide if an IMDB review is negative or positive. \n",
        "\n",
        "**Text classification** can also be set up for **multi-class classification**, where you want to classify text into more than two categories, but for the purposes of thiss introduction, we will stay with binary classification."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will begin with importing our dependencies:"
      ],
      "metadata": {
        "id": "B627Z1mHYa89"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8RZOuS9LWQvv"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBTI1bi8qdFV"
      },
      "source": [
        "## Sentiment Analysis\n",
        "\n",
        "The IMDB database contains 50,000 entries, which are evening split between positive and negative reviews. This creates a balances dataset, the benefits of this is that a model with unbalanced data is that the model will generally lean towards the more common sentiment. \n",
        "\n",
        "For example, if our training dataset contained 90% negative reviews, the model would expect to a high percentage of reviews being negative in testing or deployment. This would result in the model incorrectly identifying many positive reviews as negative, thusly decreasing model accuracy. \n",
        "\n",
        "Unlike most datasets, the IMDB dataset has already been split into a 50/50 split. This means that there are 25,000 in the training dataset and 25,000 in the testing dataset. Furthermore, each dataset is split 50/50 positive to negative reviews, meaning there are 12,500 positive and 12,500 negative reviews in each segment of the database.\n",
        "\n",
        "While this 50/50 split of training and testing data could be a problem, however, since we have a balanced dataset, we can mitigate many problems with training from that alone.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAsKG535pHep"
      },
      "source": [
        "### Download and explore the IMDB dataset\n",
        "\n",
        "We will download the data from the stanford.edu website, and then explore the file structure. Unlike many introductory projects, this model will use a more complex ETL process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "k7ZYnuajVlFN",
        "outputId": "e7673c4f-e715-4ef1-b5cc-3f607ec59d80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "84125825/84125825 [==============================] - 9s 0us/step\n"
          ]
        }
      ],
      "source": [
        "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "\n",
        "dataset = tf.keras.utils.get_file(\"aclImdb_v1\", url,\n",
        "                                    untar=True, cache_dir='.',\n",
        "                                    cache_subdir='')\n",
        "\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code will show us the files and folders in a given path.\n",
        "\n",
        "We can see several files, \"imdbEr.txt\", and folders, \"train\", \"test\", etc. "
      ],
      "metadata": {
        "id": "hCFE1M0Aahrt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "355CfOvsV1pl",
        "outputId": "db65757e-aec4-40ca-e58e-b2ab90d5823a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['imdbEr.txt', 'README', 'test', 'imdb.vocab', 'train']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "os.listdir(dataset_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will create a training directory called \"train\", and then take a look at this new directory to see if we can find the key folders or files we need."
      ],
      "metadata": {
        "id": "CNdqeNKibTPm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7ASND15oXpF1",
        "outputId": "40ef5544-cd04-48ab-923f-a583cb84509d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['urls_neg.txt',\n",
              " 'labeledBow.feat',\n",
              " 'neg',\n",
              " 'urls_pos.txt',\n",
              " 'unsupBow.feat',\n",
              " 'unsup',\n",
              " 'pos',\n",
              " 'urls_unsup.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "train_dir = os.path.join(dataset_dir, 'train')\n",
        "os.listdir(train_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysMNMI1CWDFD"
      },
      "source": [
        "The `pos` and `neg` directories contain the text files, each of which is a single movie review.\n",
        "\n",
        "<br>\n",
        "\n",
        "We can take a look at any of these reviews, which is alwayss a great idea because it helpss us get more familiar with the data as well as perform some QA on the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "R7g8hFvzWLIZ",
        "outputId": "c7b806c7-86b6-4ed1-ab34-bc368ecdbda2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rachel Griffiths writes and directs this award winning short film. A heartwarming story about coping with grief and cherishing the memory of those we've loved and lost. Although, only 15 minutes long, Griffiths manages to capture so much emotion and truth onto film in the short space of time. Bud Tingwell gives a touching performance as Will, a widower struggling to cope with his wife's death. Will is confronted by the harsh reality of loneliness and helplessness as he proceeds to take care of Ruth's pet cow, Tulip. The film displays the grief and responsibility one feels for those they have loved and lost. Good cinematography, great direction, and superbly acted. It will bring tears to all those who have lost a loved one, and survived.\n"
          ]
        }
      ],
      "source": [
        "sample_file = os.path.join(train_dir, 'pos/1181_9.txt')\n",
        "with open(sample_file) as f:\n",
        "  print(f.read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mk20TEm6ZRFP"
      },
      "source": [
        "### Load the dataset\n",
        "\n",
        "We will now load the data off disk and prepare it into a format suitable for training. \n",
        "\n",
        "<br>\n",
        "\n",
        "To do so, you will use the helpful ***text_dataset_from_directory*** utility, which expects a directory structure as follows:\n",
        "\n",
        "```\n",
        "main_directory/\n",
        "...class_a/\n",
        "......a_text_1.txt\n",
        "......a_text_2.txt\n",
        "...class_b/\n",
        "......b_text_1.txt\n",
        "......b_text_2.txt\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQauv38Lnok3"
      },
      "source": [
        "So what does this all mean? \n",
        "\n",
        "Essentially, we need to create a file system that splits our two classes (binary) into `class_a` and `class_b`, which in our case is positive and negative reviews.\n",
        "\n",
        "<br>\n",
        "\n",
        "Since our data has folders we do not need for processing, we will remove these with the follow command."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VhejsClzaWfl"
      },
      "outputs": [],
      "source": [
        "remove_dir = os.path.join(train_dir, 'unsup')\n",
        "shutil.rmtree(remove_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95kkUdRoaeMw"
      },
      "source": [
        "We will now use the `text_dataset_from_directory` utility to create a labeled `tf.data.Dataset`. \n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "It is a best practice to divide our data into three segments: **train**, **validation**, and **test**. \n",
        "\n",
        "Since this dataset has already been segmented into train and test, we can create a validation dataset from the testing data. We will do an 80/20 split of the data, but you can change this as required.\n",
        "\n",
        "<br>\n",
        "\n",
        "Using the validation_split line of code below will split this data for us. Keep in mind, you will pass through the argument for the validation percentage, so in this case we will use `0.2` or 20%, but you could use `0.3` for a 70/30 split for example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nOrK-MTYaw3C",
        "outputId": "7d12824d-12fd-46d5-b47d-baa79248831d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 20000 files for training.\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "seed = 42\n",
        "\n",
        "raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    'aclImdb/train', \n",
        "    batch_size=batch_size, \n",
        "    validation_split=0.2, \n",
        "    subset='training', \n",
        "    seed=seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Y33oxOUpYkh"
      },
      "source": [
        "We can see that the previous command found 25,000 entries, and will use 20,0000 entries for training, or 80% of 25,000.\n",
        "\n",
        "\n",
        "Now we can train a model by passing a dataset directly to `model.fit`, but we can also iterate over the dataset and print out a few examples as follows, which once again is always a great idea for assuring your commands are correct and you are getting the results as expected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "51wNaPPApk1K",
        "outputId": "f0592496-e015-44e8-e021-27cbb51cbeaf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review b'\"Pandemonium\" is a horror movie spoof that comes off more stupid than funny. Believe me when I tell you, I love comedies. Especially comedy spoofs. \"Airplane\", \"The Naked Gun\" trilogy, \"Blazing Saddles\", \"High Anxiety\", and \"Spaceballs\" are some of my favorite comedies that spoof a particular genre. \"Pandemonium\" is not up there with those films. Most of the scenes in this movie had me sitting there in stunned silence because the movie wasn\\'t all that funny. There are a few laughs in the film, but when you watch a comedy, you expect to laugh a lot more than a few times and that\\'s all this film has going for it. Geez, \"Scream\" had more laughs than this film and that was more of a horror film. How bizarre is that?<br /><br />*1/2 (out of four)'\n",
            "Label 0\n",
            "Review b\"David Mamet is a very interesting and a very un-equal director. His first movie 'House of Games' was the one I liked best, and it set a series of films with characters whose perspective of life changes as they get into complicated situations, and so does the perspective of the viewer.<br /><br />So is 'Homicide' which from the title tries to set the mind of the viewer to the usual crime drama. The principal characters are two cops, one Jewish and one Irish who deal with a racially charged area. The murder of an old Jewish shop owner who proves to be an ancient veteran of the Israeli Independence war triggers the Jewish identity in the mind and heart of the Jewish detective.<br /><br />This is were the flaws of the film are the more obvious. The process of awakening is theatrical and hard to believe, the group of Jewish militants is operatic, and the way the detective eventually walks to the final violent confrontation is pathetic. The end of the film itself is Mamet-like smart, but disappoints from a human emotional perspective.<br /><br />Joe Mantegna and William Macy give strong performances, but the flaws of the story are too evident to be easily compensated.\"\n",
            "Label 0\n",
            "Review b'Great documentary about the lives of NY firefighters during the worst terrorist attack of all time.. That reason alone is why this should be a must see collectors item.. What shocked me was not only the attacks, but the\"High Fat Diet\" and physical appearance of some of these firefighters. I think a lot of Doctors would agree with me that,in the physical shape they were in, some of these firefighters would NOT of made it to the 79th floor carrying over 60 lbs of gear. Having said that i now have a greater respect for firefighters and i realize becoming a firefighter is a life altering job. The French have a history of making great documentary\\'s and that is what this is, a Great Documentary.....'\n",
            "Label 1\n"
          ]
        }
      ],
      "source": [
        "for text_batch, label_batch in raw_train_ds.take(1):\n",
        "  for i in range(3):\n",
        "    print(\"Review\", text_batch.numpy()[i])\n",
        "    print(\"Label\", label_batch.numpy()[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWq1SUIrp1a-"
      },
      "source": [
        "We can see that the reviews contain \"normal\" or raw text, such as letters, punctuation, etc. as well as some HTML tags, such as `<br/>`. If you are new to NLP, you will know that these are red flags for your data, and we will need to address this before we can train a model.\n",
        "\n",
        "After the \"Review\" we can also see a \"Label\", which are either 0 or 1 (binary). We can see clearly from the reviews above that a \"0\" is a negative review, and a \"1\" is a positive review. \n",
        "\n",
        "With this said, it may not always be this easy, and we should trust the data source when we know it is curated a such, so we will check the `class_names` property in our dataset as follows:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "MlICTG8spyO2",
        "outputId": "a79ec221-5d82-456f-dfb7-166daaa231d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label 0 corresponds to neg\n",
            "Label 1 corresponds to pos\n"
          ]
        }
      ],
      "source": [
        "print(\"Label 0 corresponds to\", raw_train_ds.class_names[0])\n",
        "print(\"Label 1 corresponds to\", raw_train_ds.class_names[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbdO39vYqdJr"
      },
      "source": [
        "We will now use the remaining 5,000 reviews from the training set for validation.\n",
        "\n",
        "<br>\n",
        "\n",
        "We will `raw_val_ds` and `raw_test_ds` for the new validation and test datasets, respsectfully."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "JsMwwhOoqjKF",
        "outputId": "a9013446-7cb5-4f83-f355-ff90d5115ed9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 5000 files for validation.\n"
          ]
        }
      ],
      "source": [
        "raw_val_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    'aclImdb/train', \n",
        "    batch_size=batch_size, \n",
        "    validation_split=0.2, \n",
        "    subset='validation', \n",
        "    seed=seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "rdSr0Nt3q_ns",
        "outputId": "b1da565c-6500-493d-ac1c-6fab9c54319e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "raw_test_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    'aclImdb/test', \n",
        "    batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJmTiO0IYAjm"
      },
      "source": [
        "## Prepare the dataset for training\n",
        "\n",
        "We wll now standardize, tokenize, and vectorize the data. Since we are using TenssorFlow, we can use the `tf.keras.layers.TextVectorization` layer directly in the model.\n",
        "\n",
        "So what does this mean?\n",
        "\n",
        "These are all data preprocessing steps that are required to pass the data through an NLP model successfully.\n",
        "\n",
        "- **Standardization**: The removal of punctuation and HTML tags, a well as converting all text to lowercase.\n",
        "\n",
        "- **Tokenization**: Is the act of splitting up the text into \"tokens\" or segments. This can be done by breaking up a paragraph into sentences, and then sentences into individual words. We will often remove stop words and other non-required parts of the text, such as \"ing\".\n",
        "\n",
        "- **Vectorization**: Is the process of converting string text into a numerical value. \n",
        "\n",
        "<br>\n",
        "\n",
        "All of these tasks can be accomplished with this layer.\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "HTML tags will not be removed by the default standardizer in the `TextVectorization` layer (which converts text to lowercase and strips punctuation by default, but doesn't strip HTML). \n",
        "\n",
        "The following is a custom standardization function to remove the HTML tags."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "SDRI_s_tX1Hk"
      },
      "outputs": [],
      "source": [
        "def custom_standardization(input_data):\n",
        "  lowercase = tf.strings.lower(input_data)\n",
        "  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
        "  return tf.strings.regex_replace(stripped_html,\n",
        "                                  '[%s]' % re.escape(string.punctuation),\n",
        "                                  '')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2d3Aw8dsUux"
      },
      "source": [
        "Next, we will create a `TextVectorization` layer. We set the `output_mode` to `int` to create unique integer indices for each token. Or in other words, we make ure each unique word has an associated unique number.\n",
        "\n",
        "<br>\n",
        "\n",
        "We will use the custom HTML function from above, and the standard split function, along with some defined constants, such as `sequence_length`, which sets our inputs to a specific length.\n",
        "\n",
        "<br> \n",
        "\n",
        "The `sequence_length` argument will either pad (add) or truncate (subtract) values from the entries to match the value we provide."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "-c76RvSzsMnX"
      },
      "outputs": [],
      "source": [
        "max_features = 10000\n",
        "sequence_length = 250\n",
        "\n",
        "vectorize_layer = layers.TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=max_features,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlFOpfF6scT6"
      },
      "source": [
        "We now need to call the `adapt` argument to fit the state of the preprocessing layer to the dataset. This will result in the model to build an index of strings to integers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAhdjK7AtroA"
      },
      "source": [
        "***Note: It's important to only use your training data when calling adapt (using the test set would leak information).***"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This following set of code will create a text-only dataset, without labels, and then we will call our `adapt` argument to create the index."
      ],
      "metadata": {
        "id": "_adWt-DQhuV9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "GH4_2ZGJsa_X"
      },
      "outputs": [],
      "source": [
        "# Make a text-only dataset (without labels), then call adapt\n",
        "train_text = raw_train_ds.map(lambda x, y: x)\n",
        "vectorize_layer.adapt(train_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHQVEFzNt-K_"
      },
      "source": [
        "To help us understand what we just did, we will create a function to see what this layer will do to some data!\n",
        "\n",
        "<br>\n",
        "\n",
        "Note: This is where it gets cool!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "SCIg_T50wOCU"
      },
      "outputs": [],
      "source": [
        "def vectorize_text(text, label):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return vectorize_layer(text), label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "XULcm6B3xQIO",
        "outputId": "d86c70a0-4ad7-488d-eb07-f3f641239f40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review: tf.Tensor(b\"I am shocked. Shocked and dismayed that the 428 of you IMDB users who voted before me have not given this film a rating of higher than 7. 7?!?? - that's a C!. If I could give FOBH a 20, I'd gladly do it. This film ranks high atop the pantheon of modern comedy, alongside Half Baked and Mallrats, as one of the most hilarious films of all time. If you know _anything_ about rap music - YOU MUST SEE THIS!! If you know nothing about rap music - learn something!, and then see this! Comparisons to 'Spinal Tap' fail to appreciate the inspired genius of this unique film. If you liked Bob Roberts, you'll love this. Watch it and vote it a 10!\", shape=(), dtype=string)\n",
            "Label: pos\n",
            "Vectorized Review: (<tf.Tensor: shape=(1, 250), dtype=int64, numpy=\n",
            "array([[  10,  237, 2350, 2350,    3,    1,   12,    2,    1,    5,   22,\n",
            "         922, 5790,   36, 5633,  153,   69,   25,   21,  340,   11,   19,\n",
            "           4,  693,    5, 1718,   70, 1131, 1131,  177,    4, 1885,   45,\n",
            "          10,   98,  193,    1,    4,  950,  450,    1,   82,    9,   11,\n",
            "          19, 3798,  331,    1,    2,    1,    5,  709,  220, 4475,  363,\n",
            "           1,    3,    1,   14,   28,    5,    2,   88,  615,   94,    5,\n",
            "          30,   58,   45,   22,  118,  228,   42, 3662,  222,   22,  219,\n",
            "          67,   11,   45,   22,  118,  155,   42, 3662,  222,  817,  138,\n",
            "           3,   92,   67,   11, 5471,    6, 5572, 4139, 1760,    6, 1111,\n",
            "           2, 1534, 1209,    5,   11,  953,   19,   45,   22,  414, 1956,\n",
            "        2985,  483,  115,   11,  103,    9,    3, 2187,    9,    4,  296,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0]])>, <tf.Tensor: shape=(), dtype=int32, numpy=1>)\n"
          ]
        }
      ],
      "source": [
        "text_batch, label_batch = next(iter(raw_train_ds))\n",
        "first_review, first_label = text_batch[1], label_batch[1]\n",
        "print(\"Review:\", first_review)\n",
        "print(\"Label:\", raw_train_ds.class_names[first_label])\n",
        "print(\"Vectorized Review:\", vectorize_text(first_review, first_label))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6u5EX0hxyNZT"
      },
      "source": [
        "The previous code block could be confusing, but the output is rather straightforward. \n",
        "\n",
        "First you can see the original review, followed by the label (pos/neg). \n",
        "\n",
        "The final output is the vectorized version of the original review. You can see here that the review was not long enough, so the `TextVectorization` layer padded (added) to the review by adding \"0\" values to the remaing required values to match 250 values. \n",
        "\n",
        "As you can see above, each token has been replaced by an integer. You can lookup the token (string) that each integer corresponds to by calling `.get_vocabulary()` on the layer."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see that each specific word has a unique value, and this becomes more apparent when you fine duplicate words in a review, such as \"I\" or \"movie\". \n",
        "\n",
        "<br>\n",
        "\n",
        "We can see what any value is by using the `.get_vocabulary()` argument, along with the nunmeric value you want to look up. \n",
        "\n",
        "<br>\n",
        "\n",
        "***Note***: *Feel free to change the below values to see different numeric-text matches.*"
      ],
      "metadata": {
        "id": "Hy24hZBtlQTB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "kRq9hTQzhVhW",
        "outputId": "cfb69dd6-8415-40b5-c409-90d5a37f8c40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1209 --->  genius\n",
            "5471 --->  comparisons\n",
            "Vocabulary size: 10000\n"
          ]
        }
      ],
      "source": [
        "print(\"1209 ---> \",vectorize_layer.get_vocabulary()[1209])\n",
        "print(\"5471 ---> \",vectorize_layer.get_vocabulary()[5471])\n",
        "print('Vocabulary size: {}'.format(len(vectorize_layer.get_vocabulary())))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XD2H6utRydGv"
      },
      "source": [
        "We have one final preprocessing step left before we can train our model.\n",
        "\n",
        "<br>\n",
        "\n",
        "We need to apply the TextVectorization layer to the `train`, `validation` and `test` datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "2zhmpeViI1iG"
      },
      "outputs": [],
      "source": [
        "train_ds = raw_train_ds.map(vectorize_text)\n",
        "val_ds = raw_val_ds.map(vectorize_text)\n",
        "test_ds = raw_test_ds.map(vectorize_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsVQyPMizjuO"
      },
      "source": [
        "### Configure the dataset for performance\n",
        "\n",
        "Unlike a lot of things we do on our computers, machine learning models often require significant amounts of compute power to perform optimally. In fact, some models are so complex that only multi-billion dollar clusters are able to run them. \n",
        "\n",
        "For this, it is important for us to optimize both our pipeline (how data moves through our model/processes) and the data we pass through. \n",
        "\n",
        "<br>\n",
        "\n",
        "`.cache()` keeps the data in the memory after it is loaded off disk. Essentially this preloads the data so that the model will not have to wait on new data to process.\n",
        "\n",
        "`.prefetch()` overlaps data preprocessing and model execution while training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "wMcs_H7izm5m"
      },
      "outputs": [],
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use the `tf.data.AUTOTUNE` argument to make us sound better when presenting our findings...Okay...maybe not.\n",
        "\n",
        "AUTOTUNE allows the model to automatically decide how much parallelization to use during the model training, aka, how often should the model be any number of the following functions: opening, loading and training. More specifically, when should it complete more than one function at a given time."
      ],
      "metadata": {
        "id": "fEQBgBDopa5l"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLC02j2g-llC"
      },
      "source": [
        "## Create the model\n",
        "\n",
        "We can now create our model!\n",
        "\n",
        "<br>\n",
        "\n",
        "We will begin by declaring how many dimensions we want our model to have, in thiss case 16."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "dkQP6in8yUBR"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 16"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now create the layers sequentially to build our classifier. \n",
        "\n",
        "\n",
        "1. `Embedding` layer: This layer takes the integer-encoded reviews and looks up an embedding vector for each word-index. These vectors are learned as the model trains. The vectors add a dimension to the output array. The resulting dimensions are: `(batch, sequence, embedding)`.\n",
        "\n",
        "2. `GlobalAveragePooling1D` layer: returns a fixed-length output vector for each example by averaging over the sequence dimension. This allows the model to handle input of variable length, in the simplest way possible.\n",
        "\n",
        "3. This fixed-length output vector is piped through a fully-connected (`Dense`) layer with 16 hidden units.\n",
        "\n",
        "4. The last layer is densely connected with a single output node, giving us the predicted classification."
      ],
      "metadata": {
        "id": "EO_N5Utjq7qQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "xpKOoWgu-llD",
        "outputId": "10ff9eab-2fd7-4fc0-95fa-a8260ff67686",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 16)          160016    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, None, 16)          0         \n",
            "                                                                 \n",
            " global_average_pooling1d (G  (None, 16)               0         \n",
            " lobalAveragePooling1D)                                          \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 160,033\n",
            "Trainable params: 160,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = tf.keras.Sequential([\n",
        "  layers.Embedding(max_features + 1, embedding_dim),\n",
        "  layers.Dropout(0.2),\n",
        "  layers.GlobalAveragePooling1D(),\n",
        "  layers.Dropout(0.2),\n",
        "  layers.Dense(1)])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4EqVWg4-llM"
      },
      "source": [
        "### Loss function and optimizer\n",
        "\n",
        "Our model still needs a loss function and optimizer for training, as these give the model an idea how to deal with incorrect and correct predictions. \n",
        "\n",
        "- **Loss Function**: Given we are using a binary classsifier, we need to use the `losses.BinaryCrossentropy` loss function.\n",
        "\n",
        "- **Optimizer**: We will use the standard `adam` optimizer for this project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Mr0GP-cQ-llN"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer='adam',\n",
        "              metrics=tf.metrics.BinaryAccuracy(threshold=0.0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35jv_fzP-llU"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "We will train the model by passing the `dataset` object through by ussing the model.fit argument. \n",
        "\n",
        "**Epoch**: Refers to the number of times the model will run through the full dataset. Please feel free to play around with this value, but for this project we will keep it at `10`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "tXSGrjWZ-llW",
        "outputId": "a2ccc6cd-40ca-49f4-83fb-08f1f30ec8dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 9s 11ms/step - loss: 0.6639 - binary_accuracy: 0.6973 - val_loss: 0.6149 - val_binary_accuracy: 0.7720\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.5487 - binary_accuracy: 0.8008 - val_loss: 0.4987 - val_binary_accuracy: 0.8224\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.4458 - binary_accuracy: 0.8447 - val_loss: 0.4206 - val_binary_accuracy: 0.8472\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.3789 - binary_accuracy: 0.8659 - val_loss: 0.3741 - val_binary_accuracy: 0.8610\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.3357 - binary_accuracy: 0.8791 - val_loss: 0.3451 - val_binary_accuracy: 0.8672\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.3045 - binary_accuracy: 0.8882 - val_loss: 0.3263 - val_binary_accuracy: 0.8702\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2818 - binary_accuracy: 0.8979 - val_loss: 0.3131 - val_binary_accuracy: 0.8710\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2621 - binary_accuracy: 0.9042 - val_loss: 0.3035 - val_binary_accuracy: 0.8752\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2457 - binary_accuracy: 0.9097 - val_loss: 0.2966 - val_binary_accuracy: 0.8778\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2310 - binary_accuracy: 0.9162 - val_loss: 0.2921 - val_binary_accuracy: 0.8798\n"
          ]
        }
      ],
      "source": [
        "epochs = 10\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As always, we want to make sure we have the highest accuracy (binary and validation) while keeping our loss values as low as possible."
      ],
      "metadata": {
        "id": "EJQ5WppLtp7I"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EEGuDVuzb5r"
      },
      "source": [
        "## Evaluate the model\n",
        "\n",
        "To make it easier to understand the output from above, the follow code block helps use better see how the model performed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "zOMKywn4zReN",
        "outputId": "34ab0e6d-93ad-4ca3-d52c-a845a73e0329",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 6s 8ms/step - loss: 0.3104 - binary_accuracy: 0.8734\n",
            "Loss:  0.3104064166545868\n",
            "Accuracy:  0.8733999729156494\n"
          ]
        }
      ],
      "source": [
        "loss, accuracy = model.evaluate(test_ds)\n",
        "\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: \", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1iEXVTR0Z2t"
      },
      "source": [
        "This fairly naive approach achieves an accuracy of about **87%**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldbQqCw2Xc1W"
      },
      "source": [
        "### Create a plot of accuracy and loss over time\n",
        "\n",
        "We can use `model.fit()`, which returns a `History` object that contains a dictionary with everything that happened during training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "-YcvZsdvWfDf",
        "outputId": "152adcbf-d79e-4c50-ec9c-fdf99519655d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['loss', 'binary_accuracy', 'val_loss', 'val_binary_accuracy'])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "history_dict = history.history\n",
        "history_dict.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_CH32qJXruI"
      },
      "source": [
        "Each of the four main values from the model training code block are recorded, aka, accuracy and training for both the training and validation datasets.\n",
        "\n",
        "This will help us see the performance overtime, and can help us see if we are overtraining our model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "2SEMeQ5YXs8z",
        "outputId": "3c9d508d-5f19-47a3-e8ce-387f61270002",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "acc = history_dict['binary_accuracy']\n",
        "val_acc = history_dict['val_binary_accuracy']\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "# \"bo\" is for \"blue dot\"\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "# b is for \"solid blue line\"\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Z3PJemLPXwz_",
        "outputId": "0d53ce2e-3f57-42f5-c2d6-30f51fa80c36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFFyCuJoXy7r"
      },
      "source": [
        "**Training Data**\n",
        "- Blue Dots\n",
        "- Accuracy increases with each epoch\n",
        "- Loss decreases with each epoch\n",
        "\n",
        "<br>\n",
        "\n",
        "**Validation Data**\n",
        "\n",
        "- Blue Line\n",
        "- Accuracy plateaus after 6 epochs\n",
        "- Loss plateaus after 7-8 epochs\n",
        "\n",
        "<br>\n",
        "\n",
        "**Conclusion**\n",
        "\n",
        "Since the validation loss and accuracy plateau before the training data, it means we are seeing overfitting from overtraining. \n",
        "\n",
        "Notice the training loss *decreases* with each epoch and the training accuracy *increases* with each epoch. This is expected when using a gradient descent optimization—it should minimize the desired quantity on every iteration.\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Given the characteristics of our datasets discussed prior, we can prevent overfitting by calling the `tf.keras.callbacks.EarlyStopping` callback argument to stop the training before overfitting occurs. \n",
        "\n",
        "*This also helps with hypertuning as you can set a high value for your epoch and allow the model decide when to stop.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-to23J3Vy5d3"
      },
      "source": [
        "## Export the model\n",
        "\n",
        "Now that we haave finished with the model, we can export the model for deployment or future use. We used the `TextVectorization` layer on the data directly, but to make deployment easier, we can add thiss layer to the model.\n",
        "\n",
        "<br>\n",
        "\n",
        "We will create a new model that has the `TextVectorization` layer added in, but with the weights we have already trained.**bold text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "FWXsMvryuZuq",
        "outputId": "f6daafbb-e365-4a3a-ca65-cc06726530e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 7s 8ms/step - loss: 0.3104 - accuracy: 0.8734\n",
            "0.8733999729156494\n"
          ]
        }
      ],
      "source": [
        "export_model = tf.keras.Sequential([\n",
        "  vectorize_layer,\n",
        "  model,\n",
        "  layers.Activation('sigmoid')\n",
        "])\n",
        "\n",
        "export_model.compile(\n",
        "    loss=losses.BinaryCrossentropy(from_logits=False), optimizer=\"adam\", metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Test it with `raw_test_ds`, which yields raw strings\n",
        "loss, accuracy = export_model.evaluate(raw_test_ds)\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwQgoN88LoEF"
      },
      "source": [
        "### Inference on new data\n",
        "\n",
        "We can test out new reviews by calling the `model.predict()` function, and then inserting your text directly (as below), or you could pass through a list in an array. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "QW355HH5L49K",
        "outputId": "7f61ddbf-34de-422e-cecc-ba0db1385ee2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 28ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.7073385 ],\n",
              "       [0.4338148 ],\n",
              "       [0.13832909],\n",
              "       [0.499325  ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "examples = [\n",
        "  \"The movie was really amazing, best movie ever!!!\",\n",
        "  \"The movie was okay.\",\n",
        "  \"The movie was terrible, literally the worst thing ever made\",\n",
        "  \"TRY YOUR OWN CUSTOM ENTRY\"\n",
        "]\n",
        "\n",
        "export_model.predict(examples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaxlpFWpzR6c"
      },
      "source": [
        "We can now see the predictions of the model on the input text reviews. \n",
        "\n",
        "The higher the value (closer to 1) the more positive the review is, and the closer the prediction is to 0, the more negative the review is. \n",
        "\n",
        "With this information, you could have the model output the predictions, and then set up buckets or segments based on these values.\n",
        "\n",
        "<br>\n",
        "\n",
        "***This is a great example of how machine learning can aid in data analysis.***"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Next Steps\n",
        "\n",
        "- Try using this approach on the [Twitter Sentiment Analysis](https://www.kaggle.com/datasets/jp797498e/twitter-entity-sentiment-analysis) dataset\n",
        "- Create a report by segmenting the test data provided above.\n",
        "- Try webscrapping for data collect"
      ],
      "metadata": {
        "id": "Ld_-w95Fuf4j"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Binary_Classification_IMDB.ipynb",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}